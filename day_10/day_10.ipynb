{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Text Analysis in Python\n",
    "\n",
    "I am not an NLP person and this is outside of my expertise, but I know enough to give a basic introduction to text analysis and text processing tools.\n",
    "\n",
    "For this tutorial + homework, I'm going to use data from Reddit. I'm getting it from PushShift, using the following code. You can run this yourself to get your own dataset (e.g., from different subreddits, or different dates). I'd recommend, however, just using the dataset that I created. I show how to import it below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from datetime import datetime\n",
    "import time\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code used to create the dataset - no need to run this code\n",
    "\n",
    "endpt = 'https://api.pushshift.io/reddit/search/submission'\n",
    "\n",
    "subreddits = ['Coronavirus', 'politics', 'aww']\n",
    "\n",
    "# Start and end date (pushshift expects these in epoch time)\n",
    "start_date = int(datetime.strptime('2021-09-11', '%Y-%m-%d').timestamp())\n",
    "end_date = int(datetime.strptime('2021-09-25', '%Y-%m-%d').timestamp())\n",
    "\n",
    "\n",
    "def get_posts(subreddit, before = end_date, after = start_date, result = None,  min_comments = 20):\n",
    "    params = {'subreddit': subreddit,\n",
    "              'num_comments': f'>{min_comments}',\n",
    "              'before': before,\n",
    "              'size': 500\n",
    "             }\n",
    "    if result == None:\n",
    "        result = []\n",
    "    r = requests.get(endpt, params=params)\n",
    "    print(r.url)\n",
    "    print(datetime.fromtimestamp(before))\n",
    "    for item in r.json()['data']:\n",
    "        created_time = item['created_utc']\n",
    "        if created_time < after: # If we've reached the earliest we want, then return\n",
    "            print(len(result))\n",
    "            return result\n",
    "        else:\n",
    "            try:\n",
    "                result.append((item['title'],item['selftext'], created_time, subreddit))\n",
    "            except KeyError:\n",
    "                print(item)\n",
    "    time.sleep(.5)\n",
    "    return get_posts(subreddit, before = created_time, result = result)\n",
    "\n",
    "\n",
    "sr_data = []\n",
    "for subreddit in subreddits:\n",
    "    new_data = get_posts(subreddit)\n",
    "    sr_data = sr_data + new_data\n",
    "sr = pd.DataFrame(sr_data, columns = ['title', 'selftext', 'date', 'subreddit'])\n",
    "sr.date = pd.to_datetime(sr.date, unit='s')\n",
    "sr.to_csv('./sr_post_data.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code to download and import the file (DO run this code)\n",
    "sr = pd.read_csv('https://github.com/jdfoote/Intro-to-Programming-and-Data-Science/blob/fall2021/resources/data/sr_post_data.csv?raw=true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the date to a datetime, and put it in the index\n",
    "sr.index = pd.to_datetime(sr.date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sr.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarization\n",
    "\n",
    "There are some simple ways to summarize text data that can be useful, without using any special NLP tools.\n",
    "\n",
    "\n",
    "For example, it can be very interesting to see how the frequency of a term changes over time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code plots the frequency of \"COVID-19\", \"Coronavirus\", and \"Trump\" each day\n",
    "\n",
    "for term in [\"COVID-19\", \"Coronavirus\", \"Trump\"]:\n",
    "    curr_df = sr.loc[sr.title.str.contains(term) | sr.selftext.str.contains(term)]\n",
    "    posts_per_day = curr_df.resample('D').size()\n",
    "    posts_per_day.plot(label = term)\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXERCISE 1\n",
    "\n",
    "Modify the code above to plot how often \"Coronavirus\" is used in each of the three subreddits over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A similar approach is dictionary-based. The most well-known version of this is [LIWC](http://liwc.wpengine.com/), but the basic idea is that you create a set of words that are associated with a construct you are interested in, and you count how often they appear.\n",
    "\n",
    "This is a very simple example of how you might do this to look for gendered words among our subreddits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we change NAs and removed/deleted to empty strings\n",
    "sr.loc[(pd.isna(sr.selftext)) | (sr.selftext.isin(['[removed]', '[deleted]'])), 'selftext'] = ''\n",
    "sr['all_text'] = sr.title + ' ' + sr.selftext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "male_words = ['he', 'his']\n",
    "female_words = ['she', 'hers']\n",
    "\n",
    "# This puts all of the text of each subreddit into lists\n",
    "def string_to_list(x):\n",
    "    return ' '.join(x).split()\n",
    "grouped_text = sr.groupby('subreddit').all_text.apply(string_to_list)\n",
    "\n",
    "# Then, we count how often each type of words appears in each subreddit\n",
    "agg = grouped_text.aggregate({'proportionMale': lambda x: sum([x.count(y) for y in male_words])/len(x),\n",
    "                        'proportionFemale': lambda x: sum([x.count(y) for y in female_words])/len(x)}\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXERCISE 2\n",
    "\n",
    "One of the trickiest parts of analysis is getting the data in the form that you want it in order to analyze/visualize it. \n",
    "\n",
    "I think a good visualization for this would be a barplot showing how often male and female word types appear for each subreddit. I'll give you the final call to produce the plot:\n",
    "\n",
    "`sns.barplot(x='subreddit', y='proportion', hue = 'word_gender', data = agg_df_long)`\n",
    "\n",
    "Now, see if you can get the data in shape so that this code actually works! :)\n",
    "\n",
    "*Hint: You'll want to use [wide to long](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.wide_to_long.html)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Example of how wide_to_long works (from https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.wide_to_long.html)\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "\n",
    "df = pd.DataFrame({'A(weekly)-2010': np.random.rand(3),\n",
    "                   'A(weekly)-2011': np.random.rand(3),\n",
    "                   'B(weekly)-2010': np.random.rand(3),\n",
    "                   'B(weekly)-2011': np.random.rand(3),\n",
    "                   'X' : np.random.randint(3, size=3)})\n",
    "df['id'] = df.index\n",
    "df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.wide_to_long(df, # The data\n",
    "                # The prefixes for the data columns. These will become column names that hold data values.\n",
    "                stubnames = ['A(weekly)', 'B(weekly)'], \n",
    "                # i is a column which uniquely identifies each row\n",
    "                i='id',\n",
    "                # j is what you want to call the prefix\n",
    "                j='year',\n",
    "                # sep is a string that is between the stubnames and the values which will go in j\n",
    "                sep='-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Exercise 2 Code\n",
    "## This code will get the df ready for pd.wide_to_long (try printing agg_df after running these to see what it looks like)\n",
    "agg_df = agg.unstack(level=0)\n",
    "agg_df = agg_df.reset_index()\n",
    "\n",
    "### Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Once you've created agg_df_long with the columns proportion and word_gender, you should be able to run this\n",
    "sns.barplot(x='subreddit', y='proportion', hue = 'word_gender', data = agg_df_long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXERCISE 3\n",
    "\n",
    "Make your own analysis, with a different set of terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF\n",
    "\n",
    "There are more complicated approaches to summarization in Python, including using LIWC (see [here](https://pypi.org/project/liwc/)).\n",
    "\n",
    "Almost all approaches are based on a \"bag of words\" approach, where the order of words is totally ignored. This is obviously a big simplification, but can often work quite well.\n",
    "\n",
    "One thing we might want to do is to differentiate groups of texts based on how often words are used. The naive way is to just count how often words appear. However, the most common words will always appear first. So, computational linguists came up with \"term frequency--inverse document frequency\" (TF-IDF). This normalizes words based on how often they appear across groups of texts. A detailed explanation with code is [here](https://towardsdatascience.com/natural-language-processing-feature-engineering-using-tf-idf-e8b9d00e7e76).\n",
    "\n",
    "There are a number of NLP / text analysis libraries in Python. The one I'm most familiar with is scikit-learn, which is a machine learning library. NLTK, SpaCy, and textblob are some of the most popular. Here is how to run TF-IDF in scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## First, we prepare the data for the TF-IDF tool.\n",
    "# We want each subreddit to be represented by a list of strings.\n",
    "# So, we take our grouped_text (which is a list of lists of words)\n",
    "# and change it into a list of three really long strings, where each\n",
    "# string is all the words that appeared for that subreddit.\n",
    "\n",
    "# This called a 'list comprehension'\n",
    "as_text = [' '.join(x) for x in grouped_text]\n",
    "\n",
    "# It is equivalent to the following for loop\n",
    "as_text = []\n",
    "for x in grouped_text:\n",
    "    as_text.append(' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Just gets the 5000 most common words\n",
    "vectorizer = TfidfVectorizer(max_features=5000, stop_words='english')\n",
    "\n",
    "tfidf_result = vectorizer.fit_transform(as_text)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "dense = tfidf_result.todense()\n",
    "denselist = dense.tolist()\n",
    "df = pd.DataFrame(denselist, columns=feature_names).transpose()\n",
    "df.columns = list(grouped_text.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This shows the values with the highest TF-IDF for r/Coronavirus\n",
    "df.sort_values('Coronavirus', ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relative frequency\n",
    "\n",
    "\n",
    "An even simpler approach that works pretty well when comparing just two \"documents\" is to rank how much more often a word appears in one rather than the other.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "politics_str = ' '.join(sr.loc[sr.subreddit == 'politics', 'all_text']).lower()\n",
    "covid_str = ' '.join(sr.loc[sr.subreddit == 'Coronavirus', 'all_text']).lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_ratios(text):\n",
    "    counts = {}\n",
    "    tot_words = 0\n",
    "    for word in text.split():\n",
    "        counts[word] = counts.get(word, 0) + 1\n",
    "        tot_words +=1\n",
    "    result = {}\n",
    "    for word, count in counts.items():\n",
    "        result[word] = count/tot_words\n",
    "    return result\n",
    "    \n",
    "    \n",
    "politics_ratio = word_ratios(politics_str)\n",
    "covid_ratio = word_ratios(covid_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio_diff = []\n",
    "for word in politics_ratio:\n",
    "    if word in covid_ratio:\n",
    "        ratio_diff.append((word, politics_ratio[word] - covid_ratio[word]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio_diff = sorted(ratio_diff, key = lambda x: x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the words that appear more often in r/Coronavirus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio_diff[:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here are those that appear more often in r/politics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio_diff[-15:][::-1] # The [::-1] just reverses the list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification\n",
    "\n",
    "Another commonly-used tool in NLP is classification. This is a \"supervised machine learning\" model, where you build a \"training set\" of items that are classified, and a machine learner uses that set to predict the classification of new items.\n",
    "\n",
    "One very common example is sentiment. In sentiment analysis, a random set of texts is manually classified as positive, neutral, or negative. This set is then used to train a classifier to predict the sentiment of unseen texts.\n",
    "\n",
    "It's beyond the scope of this class to learn how to do machine learning, but there are also pre-trained classifiers. One I found is from [textblob](https://textblob.readthedocs.io/en/dev/).\n",
    "\n",
    "NLTK also has a pre-trained classifier, trained on social media data, called VADER. That is pretty similar to what we're looking at, so this example shows how to use it.\n",
    "\n",
    "NLTK is interesting - the core is installed in Anaconda, so you should have it. However, to get various pieces to work you need to install them. So, we need to start by installing the vader lexicon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.downloader.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "def get_sentiment(sentence):\n",
    "    vs = analyzer.polarity_scores(sentence)\n",
    "    return vs['compound']\n",
    "\n",
    "sr['sentiment'] = sr.all_text.apply(get_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sr.sort_values('sentiment', ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling\n",
    "\n",
    "Finally, I'm going to show an example of topic modeling.\n",
    "\n",
    "This is complicated, both mathematically and in code. The `gensim` package has some smart defaults, and I'm showing here the most basic, simplest way to do LDA topic modelling. There is a good, slighlty more advanced [tutorial at the gensim site](https://radimrehurek.com/gensim/auto_examples/tutorials/run_lda.html).\n",
    "\n",
    "The basic idea of topic modeling is that you are trying to optimize the likelihood of a set of distributions of words over topics and topics over documents based on the documents that actually exist. The idea is that each document can be seen as being generated by a mix of topics, and we try to find the set of topics that best matches. This works best on a large set of documents, which are themselves each quite large. In this case, the posts that we have are actually quite short. Let's plot the distribution of sizes and use topic modeling on the one with the longest posts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sr['logged_post_length'] = np.log1p(sr.all_text.str.len())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.violinplot(data=sr, \n",
    "               y='logged_post_length',\n",
    "               x='subreddit'\n",
    "              );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They all look pretty similar, but r/Coronavirus is maybe a bit longer, so let's use that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Again, we need to do an NLTK download\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.parsing.preprocessing as gpp\n",
    "from gensim.corpora import Dictionary\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from pprint import pprint\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "def run_lda(docs, \n",
    "            n_topics, # How many topics to return\n",
    "            min_count = 20 # How many docs a word must appear in to be included\n",
    "           ):\n",
    "    # Split the documents into tokens. This creates a list of words for each document.\n",
    "    print(f\"Preprocessing documents...\")\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    docs = [gpp.preprocess_string(x, filters=[gpp.strip_punctuation,\n",
    "                                              gpp.strip_multiple_whitespaces,\n",
    "                                              gpp.strip_numeric,\n",
    "                                              gpp.remove_stopwords,\n",
    "                                              gpp.strip_short\n",
    "                                             ]) for x in docs]\n",
    "    \n",
    "    # Lemmatize the words\n",
    "\n",
    "    dictionary = Dictionary(docs)\n",
    "    for doc in docs:\n",
    "        dictionary.add_documents([[lemmatizer.lemmatize(token) for token in doc]])\n",
    "    dictionary.filter_extremes(no_below=min_count, no_above=0.5)\n",
    "\n",
    "    corpus = [dictionary.doc2bow(doc) for doc in docs]\n",
    "    \n",
    "    print('Number of unique tokens: %d' % len(dictionary))\n",
    "    print('Number of documents: %d' % len(corpus))\n",
    "    \n",
    "    \n",
    "    # Train LDA model\n",
    "    print(\"Running the model...\")\n",
    "    # Set training parameters.\n",
    "    num_topics = n_topics\n",
    "    chunksize = 2000\n",
    "    passes = 20\n",
    "    iterations = 400\n",
    "    eval_every = None  # Don't evaluate model perplexity, takes too much time.\n",
    "\n",
    "    # Make a index to word dictionary.\n",
    "    #temp = dictionary[0]  # This is only to \"load\" the dictionary.\n",
    "    #id2word = dictionary.id2token\n",
    "\n",
    "    model = LdaModel(\n",
    "        corpus=corpus,\n",
    "        id2word=dictionary,\n",
    "        chunksize=chunksize,\n",
    "        alpha='auto',\n",
    "        eta='auto',\n",
    "        iterations=iterations,\n",
    "        num_topics=num_topics,\n",
    "        passes=passes,\n",
    "        eval_every=eval_every\n",
    "    )\n",
    "    \n",
    "    return (model, corpus, dictionary, docs)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## r/Coronavirus topics\n",
    "\n",
    "This code tries to look at the range of topics from the Coronavirus subreddit. It takes all of the different posts from just the Coronavirus subreddit and treats each one as a document. The problem is that they are almost all short (they are typically headlines of articles that are shared).\n",
    "\n",
    "Note that this may take a minute to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = sr.loc[sr.subreddit == 'Coronavirus', 'all_text']\n",
    "model, corpus, dictionary, docs = run_lda(dataset, 5, min_count = 10)\n",
    "pprint(model.print_topics())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Going back to the original documents to understand the topics\n",
    "\n",
    "The most common words in a topic are helpful but they can be misleading. In order to make sure that topics are capturing something meaningful, it's important to go back to the original documents. This code shows how to do that.\n",
    "\n",
    "First, we get the topics for each document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_topics = model.get_document_topics(corpus)\n",
    "\n",
    "doc_topics[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is basically a distribution of topics (by topic number) for each document. We have to do some work to connect this back to the documents themselves. First, let's put these into a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = []\n",
    "doc_id = 0\n",
    "# Loop through the topic distributions for each document\n",
    "for doc in doc_topics:\n",
    "    # Create a temporary dictionary for this document\n",
    "    curr_result = {}\n",
    "    # For each topic, add an entry to the dictionary\n",
    "    for topic_number, weight in doc:\n",
    "        curr_result[f\"topic_{topic_number}\"] = weight\n",
    "    # Then, add the dictionary to our list of dictionaries\n",
    "    result.append(curr_result)\n",
    "        \n",
    "# Turn the list of dictionaries into a dataframe\n",
    "doc_topic_df = pd.DataFrame(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, let's add the original documents to the dataframe. It took me a bit of wrangling to figure out how to get it in the right format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This took some \n",
    "doc_topic_df['original_text'] = dataset.reset_index().all_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can sort by each topic, and show the text associated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_topic_df.sort_values('topic_0', ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_topic_df.sort_values('topic_4', ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jupyter automatically truncates the text, but to see the full text, we can look at the document based on the index number (the far-left column in the dataframe). For example, this is document 269"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[269]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing results\n",
    "\n",
    "There's a cool tool called LDAVis that previous students found, and that works well with gensim models. Here's how you run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "    \n",
    "pyLDAvis.enable_notebook()\n",
    "vis = gensimvis.prepare(model, corpus, dictionary)\n",
    "pyLDAvis.display(vis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding the number of topics\n",
    "\n",
    "One approach to choosing _k_, the number of topics, is topic coherence. Here is one example of how to calculate this for many values of k. You'd then find the max, and choose that number of topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "coherence = []\n",
    "for k in range(5,20):\n",
    "    print(f'Running with {k} topics')\n",
    "    model, corpus, dictionary, docs = run_lda(dataset, n_topics=k)\n",
    "    coherence_model = CoherenceModel(model=model, \n",
    "                               texts=docs, \n",
    "                               dictionary=dictionary, \n",
    "                               coherence='c_v')\n",
    "    coherence.append((coherence_model.get_coherence(), k))\n",
    "print(coherence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(coherence, reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXERCISE 4\n",
    "\n",
    "Where topic modeling really shines is in analyzing longer texts - for example, the subreddit [changemyview](https://www.reddit.com/r/changemyview/) has fairly long posts where people explain a controversial view that they hold.\n",
    "\n",
    "Try to figure out how to get a few hundred posts from changemyview, and run a topic model on them, where the selftext of each post is a document."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "teaching",
   "language": "python",
   "name": "teaching"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
