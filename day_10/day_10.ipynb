{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Text Analysis in Python\n",
    "\n",
    "I am not an NLP person and this is outside of my expertise, but I know enough to give a basic introduction to text analysis and text processing tools.\n",
    "\n",
    "For this tutorial + homework, I'm going to use data from Reddit. I'm getting it from PushShift, using the following code. You can run this yourself to get your own dataset (e.g., from different subreddits, or different dates). I'd recommend, however, just using the dataset that I created. I show how to import it below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from datetime import datetime\n",
    "import time\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code used to create the dataset - no need to run this code\n",
    "\n",
    "endpt = 'https://api.pushshift.io/reddit/search/submission'\n",
    "\n",
    "subreddits = ['AskReddit','Coronavirus', 'politics']\n",
    "\n",
    "# Start and end date (pushshift expects these in epoch time)\n",
    "start_date = int(datetime.strptime('2020-03-11', '%Y-%m-%d').timestamp())\n",
    "end_date = int(datetime.strptime('2020-03-25', '%Y-%m-%d').timestamp())\n",
    "\n",
    "\n",
    "def get_posts(subreddit, before = end_date, after = start_date, result = None,  min_comments = 20):\n",
    "    params = {'subreddit': subreddit,\n",
    "              'num_comments': f'>{min_comments}',\n",
    "              'before': before,\n",
    "              'size': 500\n",
    "             }\n",
    "    if result == None:\n",
    "        result = []\n",
    "    r = requests.get(endpt, params=params)\n",
    "    print(r.url)\n",
    "    print(datetime.fromtimestamp(before))\n",
    "    for item in r.json()['data']:\n",
    "        created_time = item['created_utc']\n",
    "        if created_time < after: # If we've reached the earliest we want, then return\n",
    "            print(len(result))\n",
    "            return result\n",
    "        else:\n",
    "            try:\n",
    "                result.append((item['title'],item['selftext'], created_time, subreddit))\n",
    "            except KeyError:\n",
    "                print(item)\n",
    "    time.sleep(.5)\n",
    "    return get_posts(subreddit, before = created_time, result = result)\n",
    "\n",
    "\n",
    "sr_data = []\n",
    "for subreddit in subreddits:\n",
    "    new_data = get_posts(subreddit)\n",
    "    sr_data = sr_data + new_data\n",
    "sr = pd.DataFrame(sr_data, columns = ['title', 'selftext', 'date', 'subreddit'])\n",
    "sr.date = pd.to_datetime(sr.date, unit='s')\n",
    "sr.to_csv('./sr_post_data.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code to download and import the file (DO run this code)\n",
    "sr = pd.read_csv('https://github.com/jdfoote/Intro-to-Programming-and-Data-Science/blob/master/resources/data/sr_post_data.csv?raw=true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the date to a datetime, and put it in the index\n",
    "sr.index = pd.to_datetime(sr.date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>date</th>\n",
       "      <th>subreddit</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2020-03-25 03:58:28</th>\n",
       "      <td>What’s your least favorite color? Why?</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020-03-25 03:58:28</td>\n",
       "      <td>AskReddit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-25 03:57:58</th>\n",
       "      <td>What was the reason for the last time you cried?</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020-03-25 03:57:58</td>\n",
       "      <td>AskReddit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-25 03:57:17</th>\n",
       "      <td>What are songs that have to be played full bla...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020-03-25 03:57:17</td>\n",
       "      <td>AskReddit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-25 03:55:50</th>\n",
       "      <td>How to get rid of excessive amounts of anger a...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020-03-25 03:55:50</td>\n",
       "      <td>AskReddit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-25 03:51:49</th>\n",
       "      <td>What's your go to fantasy?</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020-03-25 03:51:49</td>\n",
       "      <td>AskReddit</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                 title  \\\n",
       "date                                                                     \n",
       "2020-03-25 03:58:28             What’s your least favorite color? Why?   \n",
       "2020-03-25 03:57:58   What was the reason for the last time you cried?   \n",
       "2020-03-25 03:57:17  What are songs that have to be played full bla...   \n",
       "2020-03-25 03:55:50  How to get rid of excessive amounts of anger a...   \n",
       "2020-03-25 03:51:49                         What's your go to fantasy?   \n",
       "\n",
       "                    selftext                 date  subreddit  \n",
       "date                                                          \n",
       "2020-03-25 03:58:28      NaN  2020-03-25 03:58:28  AskReddit  \n",
       "2020-03-25 03:57:58      NaN  2020-03-25 03:57:58  AskReddit  \n",
       "2020-03-25 03:57:17      NaN  2020-03-25 03:57:17  AskReddit  \n",
       "2020-03-25 03:55:50      NaN  2020-03-25 03:55:50  AskReddit  \n",
       "2020-03-25 03:51:49      NaN  2020-03-25 03:51:49  AskReddit  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sr.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarization\n",
    "\n",
    "There are some simple ways to summarize text data that can be useful, without using any special NLP tools.\n",
    "\n",
    "\n",
    "For example, it can be very interesting to see how the frequency of a term changes over time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# This code plots the frequency of \"COVID-19\", \"Coronavirus\", and \"Wuhan\" each day\n",
    "\n",
    "for term in [\"COVID-19\", \"Coronavirus\", \"Wuhan\"]:\n",
    "    curr_df = sr.loc[sr.title.str.contains(term) | sr.selftext.str.contains(term)]\n",
    "    posts_per_day = curr_df.resample('D').size()\n",
    "    posts_per_day.plot(label = term)\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXERCISE 1\n",
    "\n",
    "Modify the code above to plot how often \"Coronavirus\" is used in each of the three subreddits over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A similar approach is dictionary-based. The most well-known version of this is [LIWC](http://liwc.wpengine.com/), but the basic idea is that you create a set of words that are associated with a construct you are interested in, and you count how often they appear.\n",
    "\n",
    "This is a very simple example of how you might do this to look for gendered words among our subreddits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we change NAs and removed/deleted to empty strings\n",
    "sr.loc[(pd.isna(sr.selftext)) | (sr.selftext.isin(['[removed]', '[deleted]'])), 'selftext'] = ''\n",
    "sr['all_text'] = sr.title + ' ' + sr.selftext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "male_words = ['he', 'his']\n",
    "female_words = ['she', 'hers']\n",
    "\n",
    "# This puts all of the text of each subreddit into lists\n",
    "def string_to_list(x):\n",
    "    return ' '.join(x).split()\n",
    "grouped_text = sr.groupby('subreddit').all_text.apply(string_to_list)\n",
    "\n",
    "# Then, we count how often each type of words appears in each subreddit\n",
    "agg = grouped_text.aggregate({'proportionMale': lambda x: sum([x.count(y) for y in male_words])/len(x),\n",
    "                        'proportionFemale': lambda x: sum([x.count(y) for y in female_words])/len(x)}\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                  subreddit  \n",
       "proportionMale    AskReddit      0.000858\n",
       "                  Coronavirus    0.001704\n",
       "                  politics       0.005122\n",
       "proportionFemale  AskReddit      0.000199\n",
       "                  Coronavirus    0.000253\n",
       "                  politics       0.000126\n",
       "Name: all_text, dtype: float64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXERCISE 2\n",
    "\n",
    "One of the trickiest parts of analysis is getting the data in the form that you want it in order to analyze/visualize it. \n",
    "\n",
    "I think a good visualization for this would be a barplot showing how often male and female word types appear for each subreddit. I'll give you the final call to produce the plot:\n",
    "\n",
    "`sns.barplot(x='subreddit', y='proportion', hue = 'word_gender', data = agg_df_long)`\n",
    "\n",
    "Now, see if you can get the data in shape so that this code actually works! :)\n",
    "\n",
    "*Hint: You'll want to use [wide to long](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.wide_to_long.html)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A(weekly)-2010</th>\n",
       "      <th>A(weekly)-2011</th>\n",
       "      <th>B(weekly)-2010</th>\n",
       "      <th>B(weekly)-2011</th>\n",
       "      <th>X</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.548814</td>\n",
       "      <td>0.544883</td>\n",
       "      <td>0.437587</td>\n",
       "      <td>0.383442</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.715189</td>\n",
       "      <td>0.423655</td>\n",
       "      <td>0.891773</td>\n",
       "      <td>0.791725</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.602763</td>\n",
       "      <td>0.645894</td>\n",
       "      <td>0.963663</td>\n",
       "      <td>0.528895</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   A(weekly)-2010  A(weekly)-2011  B(weekly)-2010  B(weekly)-2011  X  id\n",
       "0        0.548814        0.544883        0.437587        0.383442  0   0\n",
       "1        0.715189        0.423655        0.891773        0.791725  1   1\n",
       "2        0.602763        0.645894        0.963663        0.528895  1   2"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Example of how wide_to_long works (from https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.wide_to_long.html)\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "\n",
    "df = pd.DataFrame({'A(weekly)-2010': np.random.rand(3),\n",
    "                   'A(weekly)-2011': np.random.rand(3),\n",
    "                   'B(weekly)-2010': np.random.rand(3),\n",
    "                   'B(weekly)-2011': np.random.rand(3),\n",
    "                   'X' : np.random.randint(3, size=3)})\n",
    "df['id'] = df.index\n",
    "df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>X</th>\n",
       "      <th>A(weekly)</th>\n",
       "      <th>B(weekly)</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th>year</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <th>2010</th>\n",
       "      <td>0</td>\n",
       "      <td>0.548814</td>\n",
       "      <td>0.437587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <th>2010</th>\n",
       "      <td>1</td>\n",
       "      <td>0.715189</td>\n",
       "      <td>0.891773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <th>2010</th>\n",
       "      <td>1</td>\n",
       "      <td>0.602763</td>\n",
       "      <td>0.963663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <th>2011</th>\n",
       "      <td>0</td>\n",
       "      <td>0.544883</td>\n",
       "      <td>0.383442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <th>2011</th>\n",
       "      <td>1</td>\n",
       "      <td>0.423655</td>\n",
       "      <td>0.791725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <th>2011</th>\n",
       "      <td>1</td>\n",
       "      <td>0.645894</td>\n",
       "      <td>0.528895</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         X  A(weekly)  B(weekly)\n",
       "id year                         \n",
       "0  2010  0   0.548814   0.437587\n",
       "1  2010  1   0.715189   0.891773\n",
       "2  2010  1   0.602763   0.963663\n",
       "0  2011  0   0.544883   0.383442\n",
       "1  2011  1   0.423655   0.791725\n",
       "2  2011  1   0.645894   0.528895"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.wide_to_long(df, # The data\n",
    "                # The prefixes for the data columns. These will become column names that hold data values.\n",
    "                stubnames = ['A(weekly)', 'B(weekly)'], \n",
    "                # i is a column which uniquely identifies each row\n",
    "                i='id',\n",
    "                # j is what you want to call the prefix\n",
    "                j='year',\n",
    "                # sep is a string that it between the stubnames and the values which will go in j\n",
    "                sep='-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Exercise 2 Code\n",
    "## This code will get the df ready for pd.wide_to_long (try printing agg_df after running these to see what it looks like)\n",
    "agg_df = agg.unstack(level=0)\n",
    "agg_df = agg_df.reset_index()\n",
    "\n",
    "### Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>proportionMale</th>\n",
       "      <th>proportionFemale</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AskReddit</td>\n",
       "      <td>0.000858</td>\n",
       "      <td>0.000199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Coronavirus</td>\n",
       "      <td>0.001704</td>\n",
       "      <td>0.000253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>politics</td>\n",
       "      <td>0.005122</td>\n",
       "      <td>0.000126</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     subreddit  proportionMale  proportionFemale\n",
       "0    AskReddit        0.000858          0.000199\n",
       "1  Coronavirus        0.001704          0.000253\n",
       "2     politics        0.005122          0.000126"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agg_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Once you've created agg_df_long with the columns proportion and word_gender, you should be able to run this\n",
    "sns.barplot(x='subreddit', y='proportion', hue = 'word_gender', data = agg_df_long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXERCISE 3\n",
    "\n",
    "Make your own analysis, with a different set of terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF\n",
    "\n",
    "There are more complicated approaches to summarization in Python, including using LIWC (see [here](https://pypi.org/project/liwc/)).\n",
    "\n",
    "Almost all approaches are based on a \"bag of words\" approach, where the order of words is totally ignored. This is obviously a big simplification, but can often work quite well.\n",
    "\n",
    "One thing we might want to do is to differentiate groups of texts based on how often words are used. The naive way is to just count how often words appear. However, the most common words will always appear first. So, computational linguists came up with \"term frequency--inverse document frequency\" (TF-IDF). This normalizes words based on how often they appear across groups of texts. A detailed explanation with code is [here](https://towardsdatascience.com/natural-language-processing-feature-engineering-using-tf-idf-e8b9d00e7e76).\n",
    "\n",
    "There are a number of NLP / text analysis libraries in Python. The one I'm most familiar with is scikit-learn, which is a machine learning library. NLTK, SpaCy, and textblob are some of the most popular. Here is how to run TF-IDF in scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "## First, we prepare the data for the TF-IDF tool.\n",
    "# We want each subreddit to be represented by a list of strings.\n",
    "# So, we take our grouped_text (which is a list of lists of words)\n",
    "# and change it into a list of three really long strings, where each\n",
    "# string is all the words that appeared for that subreddit.\n",
    "\n",
    "# This called a 'list comprehension'\n",
    "as_text = [' '.join(x) for x in grouped_text]\n",
    "\n",
    "# It is equivalent to the following for loop\n",
    "as_text = []\n",
    "for x in grouped_text:\n",
    "    as_text.append(' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Just gets the 5000 most common words\n",
    "vectorizer = TfidfVectorizer(max_features=5000, stop_words='english')\n",
    "\n",
    "tfidf_result = vectorizer.fit_transform(as_text)\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "dense = tfidf_result.todense()\n",
    "denselist = dense.tolist()\n",
    "df = pd.DataFrame(denselist, columns=feature_names).transpose()\n",
    "df.columns = ['AskReddit','Coronavirus', 'politics']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AskReddit</th>\n",
       "      <th>Coronavirus</th>\n",
       "      <th>politics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>coronavirus</th>\n",
       "      <td>0.075605</td>\n",
       "      <td>0.743420</td>\n",
       "      <td>0.688757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trump</th>\n",
       "      <td>0.061446</td>\n",
       "      <td>0.020280</td>\n",
       "      <td>0.542152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>biden</th>\n",
       "      <td>0.018434</td>\n",
       "      <td>0.000934</td>\n",
       "      <td>0.188269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sanders</th>\n",
       "      <td>0.007480</td>\n",
       "      <td>0.000934</td>\n",
       "      <td>0.146084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>says</th>\n",
       "      <td>0.007747</td>\n",
       "      <td>0.103134</td>\n",
       "      <td>0.122909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bernie</th>\n",
       "      <td>0.010686</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.092442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pandemic</th>\n",
       "      <td>0.087092</td>\n",
       "      <td>0.062174</td>\n",
       "      <td>0.088536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>joe</th>\n",
       "      <td>0.007747</td>\n",
       "      <td>0.001201</td>\n",
       "      <td>0.080203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>president</th>\n",
       "      <td>0.026715</td>\n",
       "      <td>0.017345</td>\n",
       "      <td>0.076557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>virus</th>\n",
       "      <td>0.088428</td>\n",
       "      <td>0.092060</td>\n",
       "      <td>0.066662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>response</th>\n",
       "      <td>0.006679</td>\n",
       "      <td>0.026684</td>\n",
       "      <td>0.065100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>house</th>\n",
       "      <td>0.025914</td>\n",
       "      <td>0.028552</td>\n",
       "      <td>0.065100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>senate</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012198</td>\n",
       "      <td>0.055997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>crisis</th>\n",
       "      <td>0.014693</td>\n",
       "      <td>0.024416</td>\n",
       "      <td>0.051820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>covid</th>\n",
       "      <td>0.057438</td>\n",
       "      <td>0.202666</td>\n",
       "      <td>0.051559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>americans</th>\n",
       "      <td>0.054232</td>\n",
       "      <td>0.024549</td>\n",
       "      <td>0.051559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.058240</td>\n",
       "      <td>0.202933</td>\n",
       "      <td>0.051038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>democrats</th>\n",
       "      <td>0.005877</td>\n",
       "      <td>0.002668</td>\n",
       "      <td>0.050778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>white</th>\n",
       "      <td>0.012556</td>\n",
       "      <td>0.020814</td>\n",
       "      <td>0.050257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>health</th>\n",
       "      <td>0.011488</td>\n",
       "      <td>0.064042</td>\n",
       "      <td>0.046091</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             AskReddit  Coronavirus  politics\n",
       "coronavirus   0.075605     0.743420  0.688757\n",
       "trump         0.061446     0.020280  0.542152\n",
       "biden         0.018434     0.000934  0.188269\n",
       "sanders       0.007480     0.000934  0.146084\n",
       "says          0.007747     0.103134  0.122909\n",
       "bernie        0.010686     0.000400  0.092442\n",
       "pandemic      0.087092     0.062174  0.088536\n",
       "joe           0.007747     0.001201  0.080203\n",
       "president     0.026715     0.017345  0.076557\n",
       "virus         0.088428     0.092060  0.066662\n",
       "response      0.006679     0.026684  0.065100\n",
       "house         0.025914     0.028552  0.065100\n",
       "senate        0.000000     0.012198  0.055997\n",
       "crisis        0.014693     0.024416  0.051820\n",
       "covid         0.057438     0.202666  0.051559\n",
       "americans     0.054232     0.024549  0.051559\n",
       "19            0.058240     0.202933  0.051038\n",
       "democrats     0.005877     0.002668  0.050778\n",
       "white         0.012556     0.020814  0.050257\n",
       "health        0.011488     0.064042  0.046091"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This shows the values with the highest TF-IDF for r/Coronavirus\n",
    "df.sort_values('politics', ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relative frequency\n",
    "\n",
    "\n",
    "An even simpler approach that works pretty well when comparing just two \"documents\" is to rank how much more often a word appears in one rather than the other.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "politics_str = ' '.join(sr.loc[sr.subreddit == 'politics', 'all_text']).lower()\n",
    "covid_str = ' '.join(sr.loc[sr.subreddit == 'Coronavirus', 'all_text']).lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def word_ratios(text):\n",
    "    counts = {}\n",
    "    tot_words = 0\n",
    "    for word in text.split():\n",
    "        counts[word] = counts.get(word, 0) + 1\n",
    "        tot_words +=1\n",
    "    result = {}\n",
    "    for word, count in counts.items():\n",
    "        result[word] = count/tot_words\n",
    "    return result\n",
    "    \n",
    "    \n",
    "politics_ratio = word_ratios(politics_str)\n",
    "covid_ratio = word_ratios(covid_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio_diff = []\n",
    "for word in politics_ratio:\n",
    "    if word in covid_ratio:\n",
    "        ratio_diff.append((word, politics_ratio[word] - covid_ratio[word]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio_diff = sorted(ratio_diff, key = lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('in', -0.008641070668347078),\n",
       " ('of', -0.007505213147418767),\n",
       " ('cases', -0.006393400232430248),\n",
       " ('and', -0.005384970990780979),\n",
       " ('new', -0.00411425259549587),\n",
       " ('covid-19', -0.0034109876126995915),\n",
       " ('people', -0.002588033476289512),\n",
       " ('are', -0.0025514075823793616),\n",
       " ('italy', -0.0021828066878559604),\n",
       " ('the', -0.001945542159524638)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratio_diff[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('says', 0.002419674222273823),\n",
       " ('president', 0.0025352044372583355),\n",
       " ('he', 0.002684614473721106),\n",
       " ('his', 0.002724382128075578),\n",
       " ('joe', 0.00376115500185909),\n",
       " ('bernie', 0.004146683935294064),\n",
       " ('sanders', 0.005784627280288876),\n",
       " ('biden', 0.007175764471709353),\n",
       " ('coronavirus', 0.00810381716694909),\n",
       " ('trump', 0.01965779529726481)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratio_diff[-10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification\n",
    "\n",
    "Another commonly-used tool in NLP is classification. This is a \"supervised machine learning\" model, where you build a \"training set\" of items that are classified, and a machine learner uses that set to predict the classification of new items.\n",
    "\n",
    "One very common example is sentiment. In sentiment analysis, a random set of texts is manually classified as positive, neutral, or negative. This set is then used to train a classifier to predict the sentiment of unseen texts.\n",
    "\n",
    "It's beyond the scope of this class to learn how to do machine learning, but there are also pre-trained classifiers. One I found is from [textblob](https://textblob.readthedocs.io/en/dev/).\n",
    "\n",
    "NLTK also has a pre-trained classifier, trained on social media data, called VADER. That is pretty similar to what we're looking at, so this example shows how to use it.\n",
    "\n",
    "NLTK is interesting - the core is installed in Anaconda, so you should have it. However, to get various pieces to work you need to install them. So, we need to start by installing the vader lexicon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.downloader.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "def get_sentiment(sentence):\n",
    "    vs = analyzer.polarity_scores(sentence)\n",
    "    return vs['compound']\n",
    "\n",
    "sr['sentiment'] = sr.all_text.apply(get_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>date</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>all_text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2020-03-20 14:23:51</th>\n",
       "      <td>I'm Ali Raja, MD and Shuhan He, MD emergency p...</td>\n",
       "      <td>We’re back again on the front lines of the COV...</td>\n",
       "      <td>2020-03-20 14:23:51</td>\n",
       "      <td>Coronavirus</td>\n",
       "      <td>I'm Ali Raja, MD and Shuhan He, MD emergency p...</td>\n",
       "      <td>0.9899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-16 19:15:13</th>\n",
       "      <td>“Help with the fight against SARS-CoV-2 from h...</td>\n",
       "      <td>Hello all.\\n\\nI want to inform you of the work...</td>\n",
       "      <td>2020-03-16 19:15:13</td>\n",
       "      <td>Coronavirus</td>\n",
       "      <td>“Help with the fight against SARS-CoV-2 from h...</td>\n",
       "      <td>0.9661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-23 01:47:48</th>\n",
       "      <td>You die and are at the gates of heaven. You ha...</td>\n",
       "      <td></td>\n",
       "      <td>2020-03-23 01:47:48</td>\n",
       "      <td>AskReddit</td>\n",
       "      <td>You die and are at the gates of heaven. You ha...</td>\n",
       "      <td>0.9657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-16 16:52:15</th>\n",
       "      <td>The \"What happened in your state last week?\" M...</td>\n",
       "      <td>Welcome to the 'What happened in your state la...</td>\n",
       "      <td>2020-03-16 16:52:15</td>\n",
       "      <td>politics</td>\n",
       "      <td>The \"What happened in your state last week?\" M...</td>\n",
       "      <td>0.9653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-23 17:08:02</th>\n",
       "      <td>The \"What happened in your state last week?\" M...</td>\n",
       "      <td>Welcome to the 'What happened in your state la...</td>\n",
       "      <td>2020-03-23 17:08:02</td>\n",
       "      <td>politics</td>\n",
       "      <td>The \"What happened in your state last week?\" M...</td>\n",
       "      <td>0.9653</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                 title  \\\n",
       "date                                                                     \n",
       "2020-03-20 14:23:51  I'm Ali Raja, MD and Shuhan He, MD emergency p...   \n",
       "2020-03-16 19:15:13  “Help with the fight against SARS-CoV-2 from h...   \n",
       "2020-03-23 01:47:48  You die and are at the gates of heaven. You ha...   \n",
       "2020-03-16 16:52:15  The \"What happened in your state last week?\" M...   \n",
       "2020-03-23 17:08:02  The \"What happened in your state last week?\" M...   \n",
       "\n",
       "                                                              selftext  \\\n",
       "date                                                                     \n",
       "2020-03-20 14:23:51  We’re back again on the front lines of the COV...   \n",
       "2020-03-16 19:15:13  Hello all.\\n\\nI want to inform you of the work...   \n",
       "2020-03-23 01:47:48                                                      \n",
       "2020-03-16 16:52:15  Welcome to the 'What happened in your state la...   \n",
       "2020-03-23 17:08:02  Welcome to the 'What happened in your state la...   \n",
       "\n",
       "                                    date    subreddit  \\\n",
       "date                                                    \n",
       "2020-03-20 14:23:51  2020-03-20 14:23:51  Coronavirus   \n",
       "2020-03-16 19:15:13  2020-03-16 19:15:13  Coronavirus   \n",
       "2020-03-23 01:47:48  2020-03-23 01:47:48    AskReddit   \n",
       "2020-03-16 16:52:15  2020-03-16 16:52:15     politics   \n",
       "2020-03-23 17:08:02  2020-03-23 17:08:02     politics   \n",
       "\n",
       "                                                              all_text  \\\n",
       "date                                                                     \n",
       "2020-03-20 14:23:51  I'm Ali Raja, MD and Shuhan He, MD emergency p...   \n",
       "2020-03-16 19:15:13  “Help with the fight against SARS-CoV-2 from h...   \n",
       "2020-03-23 01:47:48  You die and are at the gates of heaven. You ha...   \n",
       "2020-03-16 16:52:15  The \"What happened in your state last week?\" M...   \n",
       "2020-03-23 17:08:02  The \"What happened in your state last week?\" M...   \n",
       "\n",
       "                     sentiment  \n",
       "date                            \n",
       "2020-03-20 14:23:51     0.9899  \n",
       "2020-03-16 19:15:13     0.9661  \n",
       "2020-03-23 01:47:48     0.9657  \n",
       "2020-03-16 16:52:15     0.9653  \n",
       "2020-03-23 17:08:02     0.9653  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sr.sort_values('sentiment', ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling\n",
    "\n",
    "Finally, I'm going to show an example of topic modeling.\n",
    "\n",
    "This is complicated, both mathematically and in code. I'm pulling this example from [my book chapter](https://communitydata.science/social-media-chapter/), which was itself based on [this example](https://scikit-learn.org/stable/auto_examples/applications/plot_topics_extraction_with_nmf_lda.html#sphx-glr-auto-examples-applications-plot-topics-extraction-with-nmf-lda-py).\n",
    "\n",
    "The basic idea of topic modeling is that you are trying to optimize the likelihood of a set of distributions of words over topics and topics over documents based on the documents that actually exist. The idea is that each document can be seen as being generated by a mix of topics, and we try to find the set of topics that best matches. This works best on a large set of documents, which are themselves each quite large. Today we're making the mistake of using it on a small number of documents. I've made the opposite mistake before of using it on something like tweets. It works OK-ish in both of these circumstances, but it isn't ideal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "import sys\n",
    "import csv\n",
    "import pandas as pd\n",
    "import argparse\n",
    "\n",
    "\"\"\"\n",
    "This code was inspired/copied from http://scikit-learn.org/stable/auto_examples/applications/topics_extraction_with_nmf_lda.html.\n",
    "\n",
    "It takes in a list of documents and creates two outputs:\n",
    "1. The documents together with their topic distribution and \n",
    "2. A set of topics and the top words associated with each.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def run_lda(dataset, \n",
    "            n_topics = 10,     # How many topics to produce\n",
    "            n_features = 20000 # How many different n-grams to consider (uses the most common)\n",
    "           ):\n",
    "\n",
    "\n",
    "    print(\"Loading dataset...\")\n",
    "    t0 = time()\n",
    "    print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "    # Use tf (raw term count) features for LDA.\n",
    "    print(\"Extracting tf features for LDA...\")\n",
    "    tf_vectorizer = CountVectorizer(max_df=0.95, # Terms that show up in > max_df of documents are ignored\n",
    "                                    #min_df=2, # Terms that show up in < min_df of documents are ignored\n",
    "                                    max_features=n_features, # Only use the top max_features \n",
    "                                    stop_words='english',\n",
    "                                    ngram_range=(1,2))\n",
    "    t0 = time()\n",
    "    tf = tf_vectorizer.fit_transform(dataset)\n",
    "    print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "\n",
    "    print(\"Fitting LDA models with tf features, \"\n",
    "          \"n_samples=%d and n_features=%d...\"\n",
    "          % (len(dataset), n_features))\n",
    "    lda = LatentDirichletAllocation(n_components=n_topics, max_iter=5,\n",
    "                                    learning_method='online',\n",
    "                                    learning_offset=50.,\n",
    "                                    random_state=2017,\n",
    "                                    n_jobs=2)\n",
    "    t0 = time()\n",
    "    model = lda.fit(tf)\n",
    "    transformed_model = lda.fit_transform(tf)\n",
    "    print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "\n",
    "    # Change the values into a probability distribution for each document\n",
    "    topic_dist = [[topic/sum(topics) \n",
    "                   for topic in topics]\n",
    "                          for topics in transformed_model]\n",
    "\n",
    "    # Make the topic distribution into a dataframe\n",
    "    td = pd.DataFrame(topic_dist)\n",
    "    # Get the feature names (i.e., the words/terms)\n",
    "    tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "\n",
    "\n",
    "    # Get the top words by topic\n",
    "    topic_words = get_top_words(lda, tf_feature_names, 20)\n",
    "    # Sort by how often topic is used\n",
    "    topic_words = topic_words.reindex(sorted(topic_words.columns, \n",
    "                                             key = lambda x: td[x].sum(), \n",
    "                                             reverse=True),axis=1)\n",
    "\n",
    "    # Rearrange the columns by how often each topic is used\n",
    "    td = td.reindex(sorted(td.columns, \n",
    "                           key = lambda x: td[x].sum(), \n",
    "                           reverse=True),\n",
    "                    axis=1)\n",
    "    \n",
    "    print('Topics by how common each topic is:')\n",
    "    print(topic_words)\n",
    "    print('Distributions of topic for each subreddit')\n",
    "    print(td)\n",
    "    \n",
    "\n",
    "\n",
    "def get_top_words(model, feature_names, n_top_words):\n",
    "    '''Takes the model, the words used, and the number of words requested.\n",
    "    Returns a dataframe of the top n_top_words for each topic'''\n",
    "    r = pd.DataFrame()\n",
    "    # For each topic\n",
    "    for i, topic in enumerate(model.components_):\n",
    "        # Get the top feature names, and put them in that column\n",
    "        r[i] = [add_quotes(feature_names[i])\n",
    "                    for i in topic.argsort()[:-n_top_words - 1:-1]]\n",
    "    return r\n",
    "\n",
    "def add_quotes(s):\n",
    "    '''Adds quotes around multiple term phrases'''\n",
    "    if \" \" in s:\n",
    "        s =  '\"{}\"'.format(s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Version 1\n",
    "\n",
    "This version treats all of the text from a subreddit as a single document, and compares subreddits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "done in 0.000s.\n",
      "Extracting tf features for LDA...\n",
      "done in 1.490s.\n",
      "Fitting LDA models with tf features, n_samples=3 and n_features=20000...\n",
      "done in 1.860s.\n",
      "Topics by how common each topic is:\n",
      "                         1                9                       8  \\\n",
      "0              \"new cases\"             song                  senate   \n",
      "1   \"positive coronavirus\"  \"people reddit\"      \"amid coronavirus\"   \n",
      "2                    india        redditors    \"coronavirus crisis\"   \n",
      "3       \"amid coronavirus\"     \"video game\"  \"positive coronavirus\"   \n",
      "4                    https         username                   fauci   \n",
      "5                      nyc            girls                    poll   \n",
      "6                    mayor        favourite                     sen   \n",
      "7                   france             girl                 bailout   \n",
      "8             \"new deaths\"             nsfw               mcconnell   \n",
      "9                   county           sexual    \"coronavirus relief\"   \n",
      "10                    iran     \"men reddit\"                     cnn   \n",
      "11             ventilators        character  \"coronavirus stimulus\"   \n",
      "12                    kits         weirdest                   warns   \n",
      "13                   warns       \"thing ve\"                briefing   \n",
      "14                  closes            penis                    burr   \n",
      "15                     www           gamers                  warren   \n",
      "16             \"https www\"    \"worst thing\"         \"public health\"   \n",
      "17              \"19 cases\"       \"best way\"                illinois   \n",
      "18                    beds            anime                 mnuchin   \n",
      "19             netherlands             body                senators   \n",
      "\n",
      "                  6                       5                       3  \\\n",
      "0              song                    song               redditors   \n",
      "1         redditors               redditors         \"people reddit\"   \n",
      "2   \"people reddit\"         \"people reddit\"                    song   \n",
      "3              nsfw               favourite            \"video game\"   \n",
      "4             girls                    girl                   girls   \n",
      "5          username            \"video game\"                username   \n",
      "6         favourite                  senate                    nsfw   \n",
      "7              girl                username                weirdest   \n",
      "8          weirdest                   girls                  sexual   \n",
      "9      \"video game\"    \"coronavirus crisis\"               favourite   \n",
      "10            penis      \"amid coronavirus\"               character   \n",
      "11           gamers               character                    body   \n",
      "12           sexual                    nsfw                    girl   \n",
      "13       \"thing ve\"            \"men reddit\"            \"men reddit\"   \n",
      "14    \"worst thing\"                  sexual              \"thing ve\"   \n",
      "15            anime          \"women reddit\"  \"positive coronavirus\"   \n",
      "16             type                   anime                   penis   \n",
      "17        character                    body                   india   \n",
      "18             body  \"positive coronavirus\"                   anime   \n",
      "19       superpower                  gamers              attractive   \n",
      "\n",
      "                         4                       0                       7  \\\n",
      "0   \"positive coronavirus\"  \"positive coronavirus\"      \"amid coronavirus\"   \n",
      "1                    india      \"amid coronavirus\"             \"new cases\"   \n",
      "2              \"new cases\"                  senate                  senate   \n",
      "3       \"amid coronavirus\"             \"new cases\"  \"positive coronavirus\"   \n",
      "4                      nyc    \"coronavirus crisis\"                   india   \n",
      "5                    https                   india                  county   \n",
      "6                   county                     nyc                    iran   \n",
      "7                    mayor                    iran            \"new deaths\"   \n",
      "8     \"coronavirus crisis\"                    kits                     nyc   \n",
      "9                     kits                   mayor                   https   \n",
      "10         \"public health\"                   warns                  france   \n",
      "11                  france                   https    \"coronavirus crisis\"   \n",
      "12            \"new deaths\"         \"public health\"             ventilators   \n",
      "13                    iran             ventilators                    song   \n",
      "14    \"spread coronavirus\"                     www      \"coronavirus test\"   \n",
      "15             \"https www\"                  france                   mayor   \n",
      "16                    beds                 bailout    \"spread coronavirus\"   \n",
      "17                     mar            \"new deaths\"                    kits   \n",
      "18                    song                  amazon             \"test kits\"   \n",
      "19             ventilators                   fauci                  closes   \n",
      "\n",
      "                         2  \n",
      "0                     song  \n",
      "1          \"people reddit\"  \n",
      "2   \"positive coronavirus\"  \n",
      "3              \"new cases\"  \n",
      "4                    india  \n",
      "5                   senate  \n",
      "6                redditors  \n",
      "7                     nsfw  \n",
      "8       \"amid coronavirus\"  \n",
      "9                   sexual  \n",
      "10                   girls  \n",
      "11                   mayor  \n",
      "12            \"video game\"  \n",
      "13                     nyc  \n",
      "14                   penis  \n",
      "15    \"coronavirus crisis\"  \n",
      "16                  county  \n",
      "17                    kits  \n",
      "18                username  \n",
      "19               favourite  \n",
      "Distributions of topic for each subreddit\n",
      "          1         9         8         6         5         3         4  \\\n",
      "0  0.000003  0.999972  0.000003  0.000003  0.000003  0.000003  0.000003   \n",
      "1  0.999986  0.000002  0.000002  0.000002  0.000002  0.000002  0.000002   \n",
      "2  0.000004  0.000004  0.999964  0.000004  0.000004  0.000004  0.000004   \n",
      "\n",
      "          0         7         2  \n",
      "0  0.000003  0.000003  0.000003  \n",
      "1  0.000002  0.000002  0.000002  \n",
      "2  0.000004  0.000004  0.000004  \n"
     ]
    }
   ],
   "source": [
    "# Gets all of the strings for each subreddit and combines them into one long string for\n",
    "# that subreddit\n",
    "dataset = []\n",
    "for s in grouped_text:\n",
    "    dataset.append(' '.join(s))\n",
    "\n",
    "# Takes that list of \"documents\" and runs LDA on it\n",
    "run_lda(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Version 2\n",
    "\n",
    "This version tries to look at the range of topics from the Coronavirus subreddit. It takes all of the different posts from just the Coronavirus subreddit and treats each one as a document. In this case, the problem is that they are almost all short (they are typically headlines of articles that are shared)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "done in 0.000s.\n",
      "Extracting tf features for LDA...\n",
      "done in 0.683s.\n",
      "Fitting LDA models with tf features, n_samples=12654 and n_features=20000...\n",
      "done in 35.099s.\n",
      "Topics by how common each topic is:\n",
      "                      2                       4            5            6  \\\n",
      "0                 cases             coronavirus  coronavirus  coronavirus   \n",
      "1                    19                     000        china        masks   \n",
      "2                 covid                positive        virus         help   \n",
      "3            \"covid 19\"                pandemic        italy     hospital   \n",
      "4                   new                  tested           uk     patients   \n",
      "5           coronavirus                 testing       people      million   \n",
      "6                deaths                   tests     lockdown         care   \n",
      "7                people                    says       corona    hospitals   \n",
      "8                 total                  health      chinese        fight   \n",
      "9                 italy  \"positive coronavirus\"       travel          use   \n",
      "10            confirmed       \"tested positive\"        world      medical   \n",
      "11          \"new cases\"  \"coronavirus pandemic\"         time         make   \n",
      "12              reports                   death         stop         face   \n",
      "13  \"coronavirus cases\"        \"tests positive\"       spread  ventilators   \n",
      "14                 york                   virus      vaccine         sars   \n",
      "15           \"new york\"                      50       europe      doctors   \n",
      "16                state                  people        paper        virus   \n",
      "17                  day                  crisis      country         beds   \n",
      "18               number                     100     minister      workers   \n",
      "19                 case                   cases        panic         mask   \n",
      "\n",
      "                         8            0                     3            7  \\\n",
      "0              coronavirus         home           coronavirus      schools   \n",
      "1                    house        close                people         open   \n",
      "2                    south         stay            quarantine          say   \n",
      "3                    korea        order              symptoms         test   \n",
      "4                    white  restaurants               florida         year   \n",
      "5            \"white house\"      despite                  self         need   \n",
      "6            \"south korea\"     governor                   flu  coronavirus   \n",
      "7                 response  \"stay home\"                  says         sick   \n",
      "8                    india         bars                  like          old   \n",
      "9                     days   gatherings                   cdc          man   \n",
      "10              government        place                  ohio         kits   \n",
      "11               countries       public              official       school   \n",
      "12                national    essential                  just         hand   \n",
      "13                 medical       orders                spread       police   \n",
      "14               emergency        mayor                 calls   \"year old\"   \n",
      "15                 federal       county  \"spread coronavirus\"  \"test kits\"   \n",
      "16                    says       closes                expert       taking   \n",
      "17                     big   businesses             spreading         dies   \n",
      "18                 billion          non                cruise          nyc   \n",
      "19  \"coronavirus response\"       events    \"says coronavirus\"        woman   \n",
      "\n",
      "                         1                    9  \n",
      "0              coronavirus          coronavirus  \n",
      "1                 outbreak                https  \n",
      "2                     amid                  com  \n",
      "3   \"coronavirus outbreak\"                 live  \n",
      "4                     test               stores  \n",
      "5                    hours                  www  \n",
      "6       \"amid coronavirus\"          \"https www\"  \n",
      "7                employees                 case  \n",
      "8                  workers                novel  \n",
      "9                 students               reddit  \n",
      "10                  closed  \"novel coronavirus\"  \n",
      "11                positive              service  \n",
      "12                  amazon                 2020  \n",
      "13         \"test positive\"          information  \n",
      "14              \"24 hours\"         \"www reddit\"  \n",
      "15                     pay         \"reddit com\"  \n",
      "16                    work              website  \n",
      "17                concerns                  map  \n",
      "18                      24                 data  \n",
      "19                      st                index  \n",
      "Distributions of topic for each subreddit\n",
      "              2         4         5         6         8         0         3  \\\n",
      "0      0.012501  0.012501  0.012501  0.887491  0.012501  0.012500  0.012501   \n",
      "1      0.578524  0.010003  0.010000  0.010000  0.010002  0.010000  0.010005   \n",
      "2      0.007143  0.007143  0.007143  0.935712  0.007143  0.007143  0.007143   \n",
      "3      0.010000  0.165972  0.010007  0.159046  0.010000  0.110002  0.010000   \n",
      "4      0.441669  0.009095  0.009092  0.485592  0.009091  0.009091  0.009094   \n",
      "...         ...       ...       ...       ...       ...       ...       ...   \n",
      "12649  0.009091  0.414784  0.412398  0.009092  0.009093  0.009091  0.009091   \n",
      "12650  0.009093  0.113774  0.184285  0.364379  0.009091  0.191301  0.100804   \n",
      "12651  0.006667  0.525048  0.075751  0.006667  0.006667  0.006667  0.006667   \n",
      "12652  0.585773  0.012502  0.012507  0.152466  0.012503  0.012500  0.012501   \n",
      "12653  0.389546  0.548906  0.007694  0.007692  0.007693  0.007698  0.007693   \n",
      "\n",
      "              7         1         9  \n",
      "0      0.012504  0.012501  0.012501  \n",
      "1      0.341466  0.010000  0.010000  \n",
      "2      0.007143  0.007143  0.007143  \n",
      "3      0.411575  0.103397  0.010000  \n",
      "4      0.009092  0.009092  0.009093  \n",
      "...         ...       ...       ...  \n",
      "12649  0.009091  0.109178  0.009091  \n",
      "12650  0.009091  0.009092  0.009091  \n",
      "12651  0.006667  0.284032  0.075168  \n",
      "12652  0.012500  0.174248  0.012501  \n",
      "12653  0.007692  0.007693  0.007692  \n",
      "\n",
      "[12654 rows x 10 columns]\n"
     ]
    }
   ],
   "source": [
    "dataset = sr.loc[sr.subreddit == 'Coronavirus', 'all_text']\n",
    "run_lda(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think that this is quite a bit better and you can start to see some patterns and topics emerging."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXERCISE 4\n",
    "\n",
    "Where topic modeling really shines is in analyzing longer texts - for example, the subreddit [changemyview](https://www.reddit.com/r/changemyview/) has fairly long posts where people explain a controversial view that they hold.\n",
    "\n",
    "Try to figure out how to get a few hundred posts from changemyview, and run a topic model on them, where the selftext of each post is a document."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
