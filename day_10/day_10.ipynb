{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Text Analysis in Python\n",
    "\n",
    "I am not an NLP person and this is outside of my expertise, but I know enough to give a basic introduction to text analysis and text processing tools.\n",
    "\n",
    "For this tutorial + homework, I'm going to use data from Reddit. I'm getting it from PushShift, using the following code. You can run this yourself to get your own dataset (e.g., from different subreddits, or different dates). I'd recommend, however, just using the dataset that I created. I show how to import it below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from datetime import datetime\n",
    "import time\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code used to create the dataset - no need to run this code\n",
    "\n",
    "endpt = 'https://api.pushshift.io/reddit/search/submission'\n",
    "\n",
    "subreddits = ['AskReddit','Coronavirus', 'politics']\n",
    "\n",
    "# Start and end date (pushshift expects these in epoch time)\n",
    "start_date = int(datetime.strptime('2020-03-11', '%Y-%m-%d').timestamp())\n",
    "end_date = int(datetime.strptime('2020-03-25', '%Y-%m-%d').timestamp())\n",
    "\n",
    "\n",
    "def get_posts(subreddit, before = end_date, after = start_date, result = None,  min_comments = 20):\n",
    "    params = {'subreddit': subreddit,\n",
    "              'num_comments': f'>{min_comments}',\n",
    "              'before': before,\n",
    "              'size': 500\n",
    "             }\n",
    "    if result == None:\n",
    "        result = []\n",
    "    r = requests.get(endpt, params=params)\n",
    "    print(r.url)\n",
    "    print(datetime.fromtimestamp(before))\n",
    "    for item in r.json()['data']:\n",
    "        created_time = item['created_utc']\n",
    "        if created_time < after: # If we've reached the earliest we want, then return\n",
    "            print(len(result))\n",
    "            return result\n",
    "        else:\n",
    "            try:\n",
    "                result.append((item['title'],item['selftext'], created_time, subreddit))\n",
    "            except KeyError:\n",
    "                print(item)\n",
    "    time.sleep(.5)\n",
    "    return get_posts(subreddit, before = created_time, result = result)\n",
    "\n",
    "\n",
    "sr_data = []\n",
    "for subreddit in subreddits:\n",
    "    new_data = get_posts(subreddit)\n",
    "    sr_data = sr_data + new_data\n",
    "sr = pd.DataFrame(sr_data, columns = ['title', 'selftext', 'date', 'subreddit'])\n",
    "sr.date = pd.to_datetime(sr.date, unit='s')\n",
    "sr.to_csv('./sr_post_data.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code to download and import the file (DO run this code)\n",
    "sr = pd.read_csv('https://github.com/jdfoote/Intro-to-Programming-and-Data-Science/blob/master/resources/data/sr_post_data.csv?raw=true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the date to a datetime, and put it in the index\n",
    "sr.index = pd.to_datetime(sr.date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sr.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarization\n",
    "\n",
    "There are some simple ways to summarize text data that can be useful, without using any special NLP tools.\n",
    "\n",
    "\n",
    "For example, it can be very interesting to see how the frequency of a term changes over time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code plots the frequency of \"COVID-19\", \"Coronavirus\", and \"Wuhan\" each day\n",
    "\n",
    "for term in [\"COVID-19\", \"Coronavirus\", \"Wuhan\"]:\n",
    "    curr_df = sr.loc[sr.title.str.contains(term) | sr.selftext.str.contains(term)]\n",
    "    posts_per_day = curr_df.resample('D').size()\n",
    "    posts_per_day.plot(label = term)\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXERCISE 1\n",
    "\n",
    "Modify the code above to plot how often \"Coronavirus\" is used in each of the three subreddits over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A similar approach is dictionary-based. The most well-known version of this is [LIWC](http://liwc.wpengine.com/), but the basic idea is that you create a set of words that are associated with a construct you are interested in, and you count how often they appear.\n",
    "\n",
    "This is a very simple example of how you might do this to look for gendered words among our subreddits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we change NAs and removed/deleted to empty strings\n",
    "sr.loc[(pd.isna(sr.selftext)) | (sr.selftext.isin(['[removed]', '[deleted]'])), 'selftext'] = ''\n",
    "sr['all_text'] = sr.title + ' ' + sr.selftext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "male_words = ['he', 'his']\n",
    "female_words = ['she', 'hers']\n",
    "\n",
    "# This puts all of the text of each subreddit into lists\n",
    "def string_to_list(x):\n",
    "    return ' '.join(x).split()\n",
    "grouped_text = sr.groupby('subreddit').all_text.apply(string_to_list)\n",
    "\n",
    "# Then, we count how often each type of words appears in each subreddit\n",
    "agg = grouped_text.aggregate({'proportionMale': lambda x: sum([x.count(y) for y in male_words])/len(x),\n",
    "                        'proportionFemale': lambda x: sum([x.count(y) for y in female_words])/len(x)}\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXERCISE 2\n",
    "\n",
    "One of the trickiest parts of analysis is getting the data in the form that you want it in order to analyze/visualize it. \n",
    "\n",
    "I think a good visualization for this would be a barplot showing how often male and female word types appear for each subreddit. I'll give you the final call to produce the plot:\n",
    "\n",
    "`sns.barplot(x='subreddit', y='proportion', hue = 'word_gender', data = agg_df_long)`\n",
    "\n",
    "Now, see if you can get the data in shape so that this code actually works! :)\n",
    "\n",
    "*Hint: You'll want to use [wide to long](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.wide_to_long.html)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Example of how wide_to_long works (from https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.wide_to_long.html)\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "\n",
    "df = pd.DataFrame({'A(weekly)-2010': np.random.rand(3),\n",
    "                   'A(weekly)-2011': np.random.rand(3),\n",
    "                   'B(weekly)-2010': np.random.rand(3),\n",
    "                   'B(weekly)-2011': np.random.rand(3),\n",
    "                   'X' : np.random.randint(3, size=3)})\n",
    "df['id'] = df.index\n",
    "df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.wide_to_long(df, # The data\n",
    "                # The prefixes for the data columns. These will become column names that hold data values.\n",
    "                stubnames = ['A(weekly)', 'B(weekly)'], \n",
    "                # i is a column which uniquely identifies each row\n",
    "                i='id',\n",
    "                # j is what you want to call the prefix\n",
    "                j='year',\n",
    "                # sep is a string that it between the stubnames and the values which will go in j\n",
    "                sep='-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Exercise 2 Code\n",
    "## This code will get the df ready for pd.wide_to_long (try printing agg_df after running these to see what it looks like)\n",
    "agg_df = agg.unstack(level=0)\n",
    "agg_df = agg_df.reset_index()\n",
    "\n",
    "### Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Once you've created agg_df_long with the columns proportion and word_gender, you should be able to run this\n",
    "sns.barplot(x='subreddit', y='proportion', hue = 'word_gender', data = agg_df_long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXERCISE 3\n",
    "\n",
    "Make your own analysis, with a different set of terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF\n",
    "\n",
    "There are more complicated approaches to summarization in Python, including using LIWC (see [here](https://pypi.org/project/liwc/)).\n",
    "\n",
    "Almost all approaches are based on a \"bag of words\" approach, where the order of words is totally ignored. This is obviously a big simplification, but can often work quite well.\n",
    "\n",
    "One thing we might want to do is to differentiate groups of texts based on how often words are used. The naive way is to just count how often words appear. However, the most common words will always appear first. So, computational linguists came up with \"term frequency--inverse document frequency\" (TF-IDF). This normalizes words based on how often they appear across groups of texts. A detailed explanation with code is [here](https://towardsdatascience.com/natural-language-processing-feature-engineering-using-tf-idf-e8b9d00e7e76).\n",
    "\n",
    "There are a number of NLP / text analysis libraries in Python. The one I'm most familiar with is scikit-learn, which is a machine learning library. NLTK, SpaCy, and textblob are some of the most popular. Here is how to run TF-IDF in scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## First, we prepare the data for the TF-IDF tool.\n",
    "# We want each subreddit to be represented by a list of strings.\n",
    "# So, we take our grouped_text (which is a list of lists of words)\n",
    "# and change it into a list of three really long strings, where each\n",
    "# string is all the words that appeared for that subreddit.\n",
    "\n",
    "# This called a 'list comprehension'\n",
    "as_text = [' '.join(x) for x in grouped_text]\n",
    "\n",
    "# It is equivalent to the following for loop\n",
    "as_text = []\n",
    "for x in grouped_text:\n",
    "    as_text.append(' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Just gets the 5000 most common words\n",
    "vectorizer = TfidfVectorizer(max_features=5000, stop_words='english')\n",
    "\n",
    "tfidf_result = vectorizer.fit_transform(as_text)\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "dense = tfidf_result.todense()\n",
    "denselist = dense.tolist()\n",
    "df = pd.DataFrame(denselist, columns=feature_names).transpose()\n",
    "df.columns = ['AskReddit','Coronavirus', 'politics']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This shows the values with the highest TF-IDF for r/Coronavirus\n",
    "df.sort_values('politics', ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relative frequency\n",
    "\n",
    "\n",
    "An even simpler approach that works pretty well when comparing just two \"documents\" is to rank how much more often a word appears in one rather than the other.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "politics_str = ' '.join(sr.loc[sr.subreddit == 'politics', 'all_text']).lower()\n",
    "covid_str = ' '.join(sr.loc[sr.subreddit == 'Coronavirus', 'all_text']).lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def word_ratios(text):\n",
    "    counts = {}\n",
    "    tot_words = 0\n",
    "    for word in text.split():\n",
    "        counts[word] = counts.get(word, 0) + 1\n",
    "        tot_words +=1\n",
    "    result = {}\n",
    "    for word, count in counts.items():\n",
    "        result[word] = count/tot_words\n",
    "    return result\n",
    "    \n",
    "    \n",
    "politics_ratio = word_ratios(politics_str)\n",
    "covid_ratio = word_ratios(covid_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio_diff = []\n",
    "for word in politics_ratio:\n",
    "    if word in covid_ratio:\n",
    "        ratio_diff.append((word, politics_ratio[word] - covid_ratio[word]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio_diff = sorted(ratio_diff, key = lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio_diff[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio_diff[-10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification\n",
    "\n",
    "Another commonly-used tool in NLP is classification. This is a \"supervised machine learning\" model, where you build a \"training set\" of items that are classified, and a machine learner uses that set to predict the classification of new items.\n",
    "\n",
    "One very common example is sentiment. In sentiment analysis, a random set of texts is manually classified as positive, neutral, or negative. This set is then used to train a classifier to predict the sentiment of unseen texts.\n",
    "\n",
    "It's beyond the scope of this class to learn how to do machine learning, but there are also pre-trained classifiers. One I found is from [textblob](https://textblob.readthedocs.io/en/dev/).\n",
    "\n",
    "NLTK also has a pre-trained classifier, trained on social media data, called VADER. That is pretty similar to what we're looking at, so this example shows how to use it.\n",
    "\n",
    "NLTK is interesting - the core is installed in Anaconda, so you should have it. However, to get various pieces to work you need to install them. So, we need to start by installing the vader lexicon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.downloader.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "def get_sentiment(sentence):\n",
    "    vs = analyzer.polarity_scores(sentence)\n",
    "    return vs['compound']\n",
    "\n",
    "sr['sentiment'] = sr.all_text.apply(get_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sr.sort_values('sentiment', ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling\n",
    "\n",
    "Finally, I'm going to show an example of topic modeling.\n",
    "\n",
    "This is complicated, both mathematically and in code. I'm pulling this example from [my book chapter](https://communitydata.science/social-media-chapter/), which was itself based on [this example](https://scikit-learn.org/stable/auto_examples/applications/plot_topics_extraction_with_nmf_lda.html#sphx-glr-auto-examples-applications-plot-topics-extraction-with-nmf-lda-py).\n",
    "\n",
    "The basic idea of topic modeling is that you are trying to optimize the likelihood of a set of distributions of words over topics and topics over documents based on the documents that actually exist. The idea is that each document can be seen as being generated by a mix of topics, and we try to find the set of topics that best matches. This works best on a large set of documents, which are themselves each quite large. Today we're making the mistake of using it on a small number of documents. I've made the opposite mistake before of using it on something like tweets. It works OK-ish in both of these circumstances, but it isn't ideal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "import sys\n",
    "import csv\n",
    "import pandas as pd\n",
    "import argparse\n",
    "\n",
    "\"\"\"\n",
    "This code was inspired/copied from http://scikit-learn.org/stable/auto_examples/applications/topics_extraction_with_nmf_lda.html.\n",
    "\n",
    "It takes in a list of documents and creates two outputs:\n",
    "1. The documents together with their topic distribution and \n",
    "2. A set of topics and the top words associated with each.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def run_lda(dataset, \n",
    "            n_topics = 10,     # How many topics to produce\n",
    "            n_features = 20000 # How many different n-grams to consider (uses the most common)\n",
    "           ):\n",
    "\n",
    "\n",
    "    print(\"Loading dataset...\")\n",
    "    t0 = time()\n",
    "    print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "    # Use tf (raw term count) features for LDA.\n",
    "    print(\"Extracting tf features for LDA...\")\n",
    "    tf_vectorizer = CountVectorizer(max_df=0.95, # Terms that show up in > max_df of documents are ignored\n",
    "                                    #min_df=2, # Terms that show up in < min_df of documents are ignored\n",
    "                                    max_features=n_features, # Only use the top max_features \n",
    "                                    stop_words='english',\n",
    "                                    ngram_range=(1,2))\n",
    "    t0 = time()\n",
    "    tf = tf_vectorizer.fit_transform(dataset)\n",
    "    print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "\n",
    "    print(\"Fitting LDA models with tf features, \"\n",
    "          \"n_samples=%d and n_features=%d...\"\n",
    "          % (len(dataset), n_features))\n",
    "    lda = LatentDirichletAllocation(n_components=n_topics, max_iter=5,\n",
    "                                    learning_method='online',\n",
    "                                    learning_offset=50.,\n",
    "                                    random_state=2017,\n",
    "                                    n_jobs=2)\n",
    "    t0 = time()\n",
    "    model = lda.fit(tf)\n",
    "    transformed_model = lda.fit_transform(tf)\n",
    "    print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "\n",
    "    # Change the values into a probability distribution for each document\n",
    "    topic_dist = [[topic/sum(topics) \n",
    "                   for topic in topics]\n",
    "                          for topics in transformed_model]\n",
    "\n",
    "    # Make the topic distribution into a dataframe\n",
    "    td = pd.DataFrame(topic_dist)\n",
    "    # Get the feature names (i.e., the words/terms)\n",
    "    tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "\n",
    "\n",
    "    # Get the top words by topic\n",
    "    topic_words = get_top_words(lda, tf_feature_names, 20)\n",
    "    # Sort by how often topic is used\n",
    "    topic_words = topic_words.reindex(sorted(topic_words.columns, \n",
    "                                             key = lambda x: td[x].sum(), \n",
    "                                             reverse=True),axis=1)\n",
    "\n",
    "    # Rearrange the columns by how often each topic is used\n",
    "    td = td.reindex(sorted(td.columns, \n",
    "                           key = lambda x: td[x].sum(), \n",
    "                           reverse=True),\n",
    "                    axis=1)\n",
    "    \n",
    "    print('Topics by how common each topic is:')\n",
    "    print(topic_words)\n",
    "    print('Distributions of topic for each subreddit')\n",
    "    print(td)\n",
    "    \n",
    "\n",
    "\n",
    "def get_top_words(model, feature_names, n_top_words):\n",
    "    '''Takes the model, the words used, and the number of words requested.\n",
    "    Returns a dataframe of the top n_top_words for each topic'''\n",
    "    r = pd.DataFrame()\n",
    "    # For each topic\n",
    "    for i, topic in enumerate(model.components_):\n",
    "        # Get the top feature names, and put them in that column\n",
    "        r[i] = [add_quotes(feature_names[i])\n",
    "                    for i in topic.argsort()[:-n_top_words - 1:-1]]\n",
    "    return r\n",
    "\n",
    "def add_quotes(s):\n",
    "    '''Adds quotes around multiple term phrases'''\n",
    "    if \" \" in s:\n",
    "        s =  '\"{}\"'.format(s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Version 1\n",
    "\n",
    "This version treats all of the text from a subreddit as a single document, and compares subreddits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gets all of the strings for each subreddit and combines them into one long string for\n",
    "# that subreddit\n",
    "dataset = []\n",
    "for s in grouped_text:\n",
    "    dataset.append(' '.join(s))\n",
    "\n",
    "# Takes that list of \"documents\" and runs LDA on it\n",
    "run_lda(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Version 2\n",
    "\n",
    "This version tries to look at the range of topics from the Coronavirus subreddit. It takes all of the different posts from just the Coronavirus subreddit and treats each one as a document. In this case, the problem is that they are almost all short (they are typically headlines of articles that are shared)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = sr.loc[sr.subreddit == 'Coronavirus', 'all_text']\n",
    "run_lda(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think that this is quite a bit better and you can start to see some patterns and topics emerging."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXERCISE 4\n",
    "\n",
    "Where topic modeling really shines is in analyzing longer texts - for example, the subreddit [changemyview](https://www.reddit.com/r/changemyview/) has fairly long posts where people explain a controversial view that they hold.\n",
    "\n",
    "Try to figure out how to get a few hundred posts from changemyview, and run a topic model on them, where the selftext of each post is a document."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
