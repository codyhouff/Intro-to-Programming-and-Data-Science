{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter V2 Full Archive Search\n",
    "\n",
    "This document shows how to use Tweepy to conduct a full archive search using v2 of the Twitter API.\n",
    "\n",
    "## Prep work\n",
    "\n",
    "In order to use this code, you will need to have a developer account on Twitter, with access to the Academic Research product track. Information about who is eligible and how to apply is [here](https://developer.twitter.com/en/products/twitter-api/academic-research).\n",
    "\n",
    "Once you have an account, you will need to create a new app at https://developer.twitter.com/en/portal/dashboard and generate a \"bearer token\" from the app. Copy the bearer token to your clipboard and paste it into a new file in the same directory as this file, called `twitter_authentication.py`. The entire contents of the file should look like this:\n",
    "\n",
    "```python\n",
    "bearer_token = \"YOUR BEARER TOKEN HERE\"\n",
    "```\n",
    "\n",
    "Note that you should **never** share this token with anyone else. If, for example, you are saving your work in a Git repository, make sure that you add the `twitter_authentication.py` file to your `.gitignore`.\n",
    "\n",
    "If anyone gets this token, they will have access to your Twitter account and you will need to revoke the token (from the same interface where you created it).\n",
    "\n",
    "If you've created the file successfully, then the following two blocks of code should work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "from twitter_authentication import bearer_token\n",
    "import time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = tweepy.Client(bearer_token, wait_on_rate_limit=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Search API\n",
    "\n",
    "Full documentation for searching tweets is at https://docs.tweepy.org/en/latest/client.html#search-tweets. There are a lot of different options, but here is a simple version that gets all of the \"COVID hoax\" tweets from January 10, 2021. \n",
    "\n",
    "By default the only information returned is the tweet ID and the text. Often, we will want information about authors, too. To get information about the author, you need to add the `user_fields` parameter with the fields you want as well as the `expansions = 'author_id'` parameter. \n",
    "\n",
    "To get more information about the tweet, you need the `tweet_fields` parameter. The options are shown at https://developer.twitter.com/en/docs/twitter-api/tweets/search/api-reference/get-tweets-search-all\n",
    "\n",
    "You also likely want to build a somewhat advanced query - instructions are at https://developer.twitter.com/en/docs/twitter-api/tweets/search/integrate/build-a-query. For this query, I get English language tweets that are not retweets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "hoax_tweets = []\n",
    "for response in tweepy.Paginator(client.search_all_tweets, \n",
    "                                 query = 'COVID hoax -is:retweet lang:en',\n",
    "                                 user_fields = ['username', 'public_metrics', 'description', 'location'],\n",
    "                                 tweet_fields = ['created_at', 'geo', 'public_metrics', 'text'],\n",
    "                                 expansions = 'author_id',\n",
    "                                 start_time = '2021-01-20T00:00:00Z',\n",
    "                                 end_time = '2021-01-21T00:00:00Z',\n",
    "                              max_results=500):\n",
    "    time.sleep(1)\n",
    "    hoax_tweets.append(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that I followed the best practice above of saving the raw response returned. If this were a real project, I would write out all of the raw responses into a file. For long-running queries (e.g., if you need to get hundreds of thousands of tweets), you will often want to build in some error handling and a way to resume data collection. For example, you might write all of the results to a file and then open the file, retrieve the last tweet, and use the ID of that tweet to tell the script where to start to retrieve new tweets.\n",
    "\n",
    "The other problem is that the object that is returned is a bit confusing - it is nested, with the tweet data in `.data` and the user data in `.includes['users']`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Tweet id=1352042670059737089 text=@larrycharlesism ALSO, the mitigation measures undertaken for COVID have absolutely crushed the flu in the Northern Hemisphere. So it's not equal. In a situation where flu has been reduced to a wisp, COVID has still killed 400,000. That's my argument to them of 'the hoax'.\n",
       "https://t.co/dwMlQn75jf>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hoax_tweets[0].data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<User id=862336513 name=Red Ace ðŸ§¦ username=CallOfDove>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hoax_tweets[0].includes['users'][2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that both of these are objects. The data that we asked for in `user_fields` and `tweet_fields` above are attributes of the objects. For example, here's the user's description:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Libertarian+Conservative=Me, Catholic Christian, ðŸ‡ºðŸ‡¸ ðŸ‡²ðŸ‡½ ðŸ‡µðŸ‡· , In that order Also also also typo man'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hoax_tweets[0].includes['users'][2].description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will often want to reorganize these into a flat file, which means connecting a tweet to the user data of the user who wrote it. I show an example of how to do that here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = []\n",
    "user_dict = {}\n",
    "# Loop through each response object\n",
    "for response in hoax_tweets:\n",
    "    # Take all of the users, and put them into a dictionary of dictionaries with the info we want to keep\n",
    "    for user in response.includes['users']:\n",
    "        user_dict[user.id] = {'username': user.username, \n",
    "                              'followers': user.public_metrics['followers_count'],\n",
    "                              'tweets': user.public_metrics['tweet_count'],\n",
    "                              'description': user.description,\n",
    "                              'location': user.location\n",
    "                             }\n",
    "    for tweet in response.data:\n",
    "        # For each tweet, find the author's information\n",
    "        author_info = user_dict[tweet.author_id]\n",
    "        # Put all of the information we want to keep in a single dictionary for each tweet\n",
    "        result.append({'author_id': tweet.author_id, \n",
    "                       'username': author_info['username'],\n",
    "                       'author_followers': author_info['followers'],\n",
    "                       'author_tweets': author_info['tweets'],\n",
    "                       'author_description': author_info['description'],\n",
    "                       'author_location': author_info['location'],\n",
    "                       'text': tweet.text,\n",
    "                       'created_at': tweet.created_at,\n",
    "                       'retweets': tweet.public_metrics['retweet_count'],\n",
    "                       'replies': tweet.public_metrics['reply_count'],\n",
    "                       'likes': tweet.public_metrics['like_count'],\n",
    "                       'quote_count': tweet.public_metrics['quote_count']\n",
    "                      })\n",
    "\n",
    "# Change this list of dictionaries into a dataframe\n",
    "df = pd.DataFrame(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author_id</th>\n",
       "      <th>username</th>\n",
       "      <th>author_followers</th>\n",
       "      <th>author_tweets</th>\n",
       "      <th>author_description</th>\n",
       "      <th>author_location</th>\n",
       "      <th>text</th>\n",
       "      <th>created_at</th>\n",
       "      <th>retweets</th>\n",
       "      <th>replies</th>\n",
       "      <th>likes</th>\n",
       "      <th>quote_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1173999122791055360</td>\n",
       "      <td>LardFDorkness2</td>\n",
       "      <td>1131</td>\n",
       "      <td>13548</td>\n",
       "      <td>1st LardFDorkness account banned for gettin' s...</td>\n",
       "      <td>None</td>\n",
       "      <td>@larrycharlesism ALSO, the mitigation measures...</td>\n",
       "      <td>2021-01-20 23:57:46+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>733606999</td>\n",
       "      <td>james_thomas127</td>\n",
       "      <td>10</td>\n",
       "      <td>4897</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td>@covidhoax2020 @KevinVanAusdal LOLOL your name...</td>\n",
       "      <td>2021-01-20 23:55:40+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>862336513</td>\n",
       "      <td>CallOfDove</td>\n",
       "      <td>245</td>\n",
       "      <td>13286</td>\n",
       "      <td>Libertarian+Conservative=Me, Catholic Christia...</td>\n",
       "      <td>None</td>\n",
       "      <td>@SimonMichaelPa2 @cristhian_0707 @guypbenson I...</td>\n",
       "      <td>2021-01-20 23:54:40+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>971840730137178112</td>\n",
       "      <td>WilliamMcK25thP</td>\n",
       "      <td>2985</td>\n",
       "      <td>10862</td>\n",
       "      <td>Proud AMERICAN! USMC Brat #WalkedAway on Nov 2...</td>\n",
       "      <td>Southern California</td>\n",
       "      <td>@XHNews They forgot to thank the Covid Hoax, t...</td>\n",
       "      <td>2021-01-20 23:54:21+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>798360156559982592</td>\n",
       "      <td>Since_U_Asked</td>\n",
       "      <td>1092</td>\n",
       "      <td>26133</td>\n",
       "      <td>I like ez hikes following streams\\nthen cozy n...</td>\n",
       "      <td>Obama saved us at darkest hour</td>\n",
       "      <td>400.000 soldiers buried in Arlington\\n400,000 ...</td>\n",
       "      <td>2021-01-20 23:53:59+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1011</th>\n",
       "      <td>1326533089405833216</td>\n",
       "      <td>AlwaysVoteTruth</td>\n",
       "      <td>24</td>\n",
       "      <td>7439</td>\n",
       "      <td>MPA praying for our democracy back in the USA</td>\n",
       "      <td>None</td>\n",
       "      <td>@RandPaul You meant to say the Trump regime th...</td>\n",
       "      <td>2021-01-20 00:02:58+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1012</th>\n",
       "      <td>266256718</td>\n",
       "      <td>jaze_ca</td>\n",
       "      <td>4148</td>\n",
       "      <td>94447</td>\n",
       "      <td>#mandatoryvaccinenationwide\\n#ResignDougFord\\n...</td>\n",
       "      <td>None</td>\n",
       "      <td>@DeanLuk @weeser1 @MSNBC Hey Trumpers, if Covi...</td>\n",
       "      <td>2021-01-20 00:02:00+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1013</th>\n",
       "      <td>59075703</td>\n",
       "      <td>panji90</td>\n",
       "      <td>359</td>\n",
       "      <td>13275</td>\n",
       "      <td>The A stands for APAAN TUH? (Kuis-Dang-Dut~). ...</td>\n",
       "      <td>Cempakaputihindah, Indonesia</td>\n",
       "      <td>A very good resource on Covid-19. It explained...</td>\n",
       "      <td>2021-01-20 00:01:54+00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1014</th>\n",
       "      <td>24393759</td>\n",
       "      <td>soma77</td>\n",
       "      <td>66</td>\n",
       "      <td>23554</td>\n",
       "      <td>The presenter, John Kuykendall lived as a monk...</td>\n",
       "      <td>Sparks, Nevada</td>\n",
       "      <td>Remembering Covid victims, Biden spends emotio...</td>\n",
       "      <td>2021-01-20 00:01:39+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1015</th>\n",
       "      <td>601434382</td>\n",
       "      <td>Rfotofolio</td>\n",
       "      <td>952</td>\n",
       "      <td>5560</td>\n",
       "      <td>Our mission is to give photographers access to...</td>\n",
       "      <td>United States</td>\n",
       "      <td>@Yamiche 400000 Americans dead from Covid .. t...</td>\n",
       "      <td>2021-01-20 00:01:21+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1016 rows Ã— 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                author_id         username  author_followers  author_tweets  \\\n",
       "0     1173999122791055360   LardFDorkness2              1131          13548   \n",
       "1               733606999  james_thomas127                10           4897   \n",
       "2               862336513       CallOfDove               245          13286   \n",
       "3      971840730137178112  WilliamMcK25thP              2985          10862   \n",
       "4      798360156559982592    Since_U_Asked              1092          26133   \n",
       "...                   ...              ...               ...            ...   \n",
       "1011  1326533089405833216  AlwaysVoteTruth                24           7439   \n",
       "1012            266256718          jaze_ca              4148          94447   \n",
       "1013             59075703          panji90               359          13275   \n",
       "1014             24393759           soma77                66          23554   \n",
       "1015            601434382       Rfotofolio               952           5560   \n",
       "\n",
       "                                     author_description  \\\n",
       "0     1st LardFDorkness account banned for gettin' s...   \n",
       "1                                                         \n",
       "2     Libertarian+Conservative=Me, Catholic Christia...   \n",
       "3     Proud AMERICAN! USMC Brat #WalkedAway on Nov 2...   \n",
       "4     I like ez hikes following streams\\nthen cozy n...   \n",
       "...                                                 ...   \n",
       "1011      MPA praying for our democracy back in the USA   \n",
       "1012  #mandatoryvaccinenationwide\\n#ResignDougFord\\n...   \n",
       "1013  The A stands for APAAN TUH? (Kuis-Dang-Dut~). ...   \n",
       "1014  The presenter, John Kuykendall lived as a monk...   \n",
       "1015  Our mission is to give photographers access to...   \n",
       "\n",
       "                     author_location  \\\n",
       "0                               None   \n",
       "1                               None   \n",
       "2                               None   \n",
       "3                Southern California   \n",
       "4     Obama saved us at darkest hour   \n",
       "...                              ...   \n",
       "1011                            None   \n",
       "1012                            None   \n",
       "1013    Cempakaputihindah, Indonesia   \n",
       "1014                  Sparks, Nevada   \n",
       "1015                   United States   \n",
       "\n",
       "                                                   text  \\\n",
       "0     @larrycharlesism ALSO, the mitigation measures...   \n",
       "1     @covidhoax2020 @KevinVanAusdal LOLOL your name...   \n",
       "2     @SimonMichaelPa2 @cristhian_0707 @guypbenson I...   \n",
       "3     @XHNews They forgot to thank the Covid Hoax, t...   \n",
       "4     400.000 soldiers buried in Arlington\\n400,000 ...   \n",
       "...                                                 ...   \n",
       "1011  @RandPaul You meant to say the Trump regime th...   \n",
       "1012  @DeanLuk @weeser1 @MSNBC Hey Trumpers, if Covi...   \n",
       "1013  A very good resource on Covid-19. It explained...   \n",
       "1014  Remembering Covid victims, Biden spends emotio...   \n",
       "1015  @Yamiche 400000 Americans dead from Covid .. t...   \n",
       "\n",
       "                    created_at  retweets  replies  likes  quote_count  \n",
       "0    2021-01-20 23:57:46+00:00         0        0      1            0  \n",
       "1    2021-01-20 23:55:40+00:00         0        0      0            0  \n",
       "2    2021-01-20 23:54:40+00:00         0        0      0            0  \n",
       "3    2021-01-20 23:54:21+00:00         0        0      0            0  \n",
       "4    2021-01-20 23:53:59+00:00         0        0      0            0  \n",
       "...                        ...       ...      ...    ...          ...  \n",
       "1011 2021-01-20 00:02:58+00:00         0        0      0            0  \n",
       "1012 2021-01-20 00:02:00+00:00         0        0      1            0  \n",
       "1013 2021-01-20 00:01:54+00:00         1        0      0            0  \n",
       "1014 2021-01-20 00:01:39+00:00         0        0      0            0  \n",
       "1015 2021-01-20 00:01:21+00:00         0        0      0            0  \n",
       "\n",
       "[1016 rows x 12 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `requests`-based version\n",
    "\n",
    "If you want to do things without tweepy, here is some boilerplate code that should work. As you can see, it's much more complicated. Be grateful for the tweepy developers!! :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import json\n",
    "import twitter_authentication as config\n",
    "import time\n",
    "\n",
    "# Save your bearer token in a file called twitter_authentication.py in this directory\n",
    "# Should look like this:\n",
    "# bearer_token = 'YOUR_BEARER_TOKEN_HERE'\n",
    "\n",
    "bearer_token = config.bearer_token\n",
    "query = '(#COVID) OR (#COVID-19)'\n",
    "out_file = 'raw_tweets.txt'\n",
    "\n",
    "search_url = \"https://api.twitter.com/2/tweets/search/all\"\n",
    "\n",
    "# Optional params: start_time,end_time,since_id,until_id,max_results,next_token,\n",
    "# expansions,tweet.fields,media.fields,poll.fields,place.fields,user.fields\n",
    "query_params = {'query': query,\n",
    "                'start_time': '2010-01-01T12:00:00Z',\n",
    "                'tweet.fields': 'author_id,public_metrics',\n",
    "                 'user.fields': 'username',\n",
    "                'expansions': 'author_id',\n",
    "                'max_results': 500\n",
    "               }\n",
    "\n",
    "\n",
    "def create_headers(bearer_token):\n",
    "    headers = {\"Authorization\": \"Bearer {}\".format(bearer_token)}\n",
    "    return headers\n",
    "\n",
    "\n",
    "def connect_to_endpoint(url, headers, params, next_token = None):\n",
    "    if next_token:\n",
    "        params['next_token'] = next_token\n",
    "    response = requests.request(\"GET\", search_url, headers=headers, params=params)\n",
    "    time.sleep(3.1)\n",
    "    print(response.status_code)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(response.status_code, response.text)\n",
    "    return response.json()\n",
    "\n",
    "\n",
    "def get_tweets(num_tweets, output_fh):\n",
    "    next_token = None\n",
    "    tweets_stored = 0\n",
    "    while tweets_stored < num_tweets:\n",
    "        headers = create_headers(bearer_token)\n",
    "        json_response = connect_to_endpoint(search_url, headers, query_params, next_token)\n",
    "        if json_response['meta']['result_count'] == 0:\n",
    "            break\n",
    "        author_dict = {x['id']: x['username'] for x in json_response['includes']['users']}\n",
    "        for tweet in json_response['data']:\n",
    "            try:\n",
    "                tweet['username'] = author_dict[tweet['author_id']]\n",
    "            except KeyError:\n",
    "                print(f\"No data for {tweet['author_id']}\")\n",
    "            output_fh.write(json.dumps(tweet) + '\\n')\n",
    "            tweets_stored += 1\n",
    "        try:\n",
    "            next_token = json_response['meta']['next_token']\n",
    "        except KeyError:\n",
    "            break\n",
    "    return None\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    with open(out_file, 'w') as f:\n",
    "        get_tweets(500, f)\n",
    "\n",
    "\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = []\n",
    "with open(out_file, 'r') as f:\n",
    "    for row in f.readlines():\n",
    "        tweet = json.loads(row)\n",
    "        tweets.append(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "teaching",
   "language": "python",
   "name": "teaching"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
