{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using APIs in Projects\n",
    "\n",
    "\n",
    "When getting data from APIs, I strongly suggest following a three-step workflow:\n",
    "\n",
    "1. Write some code that gets data from an API and saves all of the data (if possible) to a file\n",
    "2. Write a second program (usually a second file) that loads the data from the API, extracts the data that will be useful for analysis, and saves it in a flat file (typically a CSV).\n",
    "3. Program number 3 loads the CSV file and does the analysis\n",
    "\n",
    "This approach has a few important benefits.\n",
    "\n",
    "The first and most important is that often it is difficult to get the same raw data again. If you are using Twitter, then the Search API only lets you get the last week. If you are doing analysis a month down the road and decide that you really wish you had saved metadata about the number of retweets, it is too late. By saving the raw data you can change your measures or analysis strategy and still have access to the data.\n",
    "\n",
    "The second is that this gives you a nice pipeline, with intermediate files. Instead of including the entire raw data file in the code that does analysis, you only have to load the CSV, which is often much smaller and easier to work with.\n",
    "\n",
    "This brief lesson will show an example of this workflow, using `tweepy`.\n",
    "\n",
    "Note that I'm going to put everything in one file for convenience, but my typical workflow is to put these in separate files and then run each file separately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Program 1 - Data Retrieval\n",
    "\n",
    "The goal of our project is to produce a visualization of the histogram of the number of retweets for recent tweets about President Trump. The first program gets tweets about President Trump."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import json\n",
    "from twitter_authentication import CONSUMER_KEY, CONSUMER_SECRET, ACCESS_TOKEN, ACCESS_TOKEN_SECRET\n",
    "\n",
    "auth = tweepy.OAuthHandler(CONSUMER_KEY, CONSUMER_SECRET)\n",
    "auth.set_access_token(ACCESS_TOKEN, ACCESS_TOKEN_SECRET)\n",
    "api = tweepy.API(auth, wait_on_rate_limit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a list to store the results\n",
    "results = []\n",
    "for tweet in tweepy.Cursor(api.search, \n",
    "                           q='Trump -filter:retweets', # only get the original tweets\n",
    "                           tweet_mode = 'extended',\n",
    "                           count=200).items(5000): # Change this to as high as you like, if you have time :)\n",
    "    results.append(tweet._json)\n",
    "    print(tweet.user.screen_name + \"\\t\" + str(tweet.created_at) + \"\\t\" + tweet.full_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then, write the results to a file\n",
    "with open('raw_trump_tweets.json', 'w') as f:\n",
    "    json.dump(results, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Program 2 - Data Cleaning\n",
    "\n",
    "This program loads the saved raw data, grabs what we want, and converts it into a csv.\n",
    "\n",
    "I decided to save the timestamp, text, and retweet and favorite counts.\n",
    "\n",
    "This is also where you typically would do more complicated measure creation. Here I show how to create a measure of tweet_length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('raw_trump_tweets.json', 'r') as f:\n",
    "    tweets = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "with open('cleaned_data.csv', 'w', \n",
    "          encoding='UTF-8',\n",
    "          newline='') as fn:\n",
    "    f = csv.writer(fn)\n",
    "    f.writerow(['created_at',\n",
    "                'tweet_text',\n",
    "                'retweets',\n",
    "                'favorites',\n",
    "                'tweet_length'\n",
    "               ])\n",
    "    for tweet in tweets:\n",
    "        f.writerow([tweet['created_at'], \n",
    "                    tweet['full_text'],\n",
    "                    tweet['retweet_count'],\n",
    "                    tweet['favorite_count'],\n",
    "                    len(tweet['full_text'])\n",
    "                   ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Program 3 - Data Analysis\n",
    "\n",
    "Here we use pandas to load the data and analyze it. This could include statistical tests. Here, I'm just visualizing the distribution of retweets and the relationship between retweets and length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./cleaned_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just make sure it looks OK.\n",
    "df.sort_values('retweets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(df.retweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, it's super skewed, with most tweets never getting retweeted while a few get tons of retweets.\n",
    "\n",
    "Let's see if it changes if we get rid of the tweets that never got retweeted (like, maybe we have a principled reason to believe they are different than other tweets)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(df.loc[df.retweets > 0, 'retweets'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As I thought, this is a somewhat \"scale-free\" distribution, meaning wherever you zoom in, you see the same pattern. Try changing the `0` up above to any (small) number."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For fun, let's also look at the relationship between retweets and tweet length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.jointplot(y='retweets', x='tweet_length', data = df);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Because retweets are so skewed, let's log them\n",
    "p = sns.jointplot(y=np.log(df.retweets + 1), x='tweet_length', data = df)\n",
    "p.set_axis_labels('Tweet Length','Retweets (log)');"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
